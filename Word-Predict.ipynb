{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46abe584-ff1f-4c84-8442-4428b17ce291",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_dict_path = r'\\\\wsl$\\Ubuntu-20.04\\usr\\share\\dict\\british-english'\n",
    "per_dict_path= r'C:\\Dev\\data\\persian_words.txt'\n",
    "#per_dict_path= r'C:\\Dev\\data\\persian_word_larger.txt'\n",
    "lang_dict = {'en': en_dict_path, 'per': per_dict_path}\n",
    "encoding = {'en': 'utf-8', 'per': 'utf-8'}\n",
    "normalizer_dict = {}\n",
    "word_len = 5\n",
    "\n",
    "\n",
    "def read_file(lang='en'):\n",
    "    global lang_dict\n",
    "    with open(lang_dict[lang], 'r', encoding=encoding[lang]) as f:\n",
    "        words = f.readlines()\n",
    "        return words\n",
    "    return None\n",
    "\n",
    "\n",
    "def normalizer(word_list: list, acceptable_letter=[], not_acceptable_letter=[]) -> list:\n",
    "    word_list = [word.strip().lower() for word in word_list]\n",
    "    word_list = [word for word in word_list  if len(word) == word_len]\n",
    "    normal_words = [word for word in word_list if all(letter in acceptable_letter for letter in word)]\n",
    "    normal_words = [word for word in normal_words if not any(letter in word for letter in not_acceptable_letter)]\n",
    "    return normal_words\n",
    "\n",
    "\n",
    "def english_normalizer(word_list: list) -> list:\n",
    "    legal_letters = [chr(c) for c in range(ord('a'), ord('z')+1)]\n",
    "    return normalizer(word_list, legal_letters)\n",
    "\n",
    "\n",
    "def persian_normalizer(word_list: list) -> list:\n",
    "    legal_letters = list('ابپتثجچحخدذرزژسشصضعغفقکگلمنوهیطظآئآأؤءكژَٔ')\n",
    "    return normalizer(word_list, legal_letters)\n",
    "\n",
    "\n",
    "def letter_freqency_calc(word_list: list) -> dict:\n",
    "    freq_dict = {}\n",
    "    for word in word_list:\n",
    "        for letter in word:\n",
    "            if letter in freq_dict:\n",
    "                freq_dict[letter] += 1\n",
    "            else:\n",
    "                freq_dict[letter] = 1\n",
    "    return freq_dict\n",
    "\n",
    "\n",
    "def point_calc_(word_list:list, letter_grade: dict) -> dict:\n",
    "    n = len(word_list)\n",
    "    word_point = {word: (1.0*sum(letter_grade[l] for l in word))/n for word in word_list}\n",
    "    return word_point\n",
    "\n",
    "\n",
    "def sort_list_and_fork(word_dict: dict) -> (list, list):\n",
    "    word_dict = {word: point for word, point in sorted(word_dict.items(), key=lambda item:item[1], reverse=True)}\n",
    "    word_list = [word for word, point in word_dict.items()]\n",
    "    unique_list = [word for word, point in word_dict.items() if len(set(list(word))) == len(word)]\n",
    "    return word_list, unique_list\n",
    "\n",
    "\n",
    "def get_next_suggest(word_list:list, has=[], has_indx={}, not_have=[], skip=0):\n",
    "    latest = ''\n",
    "    for word in word_list:\n",
    "        is_letter_cond = all(letter in word for letter in has) and (not any(letter in word for letter in not_have))\n",
    "        position_cond = all(word[i] == letter for i, letter in has_indx.items())\n",
    "        if  is_letter_cond and position_cond:\n",
    "            latest = word\n",
    "            if skip == 0:\n",
    "                return word\n",
    "            else:\n",
    "                skip -= 1\n",
    "    return latest\n",
    "\n",
    "\n",
    "def process_word(word_list: list) -> (list, list):\n",
    "    letter_dict = letter_freqency_calc(word_list)\n",
    "    word_points = point_calc_(word_list, letter_dict)\n",
    "    return sort_list_and_fork(word_points)\n",
    "normalizer_dict = {'en': lambda w: english_normalizer(w), 'per': lambda w: persian_normalizer(w)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a4360bf-ee0a-4134-abfa-d044fdc17a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = 'en'\n",
    "\n",
    "word_list = read_file(lang)\n",
    "word_list = normalizer_dict[lang](word_list)\n",
    "wlist, ulist = process_word(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8672fd2d-f8b4-45df-87bd-05f617cd6020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique=\t \n",
      "Guess=\t loony\n"
     ]
    }
   ],
   "source": [
    "has = list()\n",
    "has_indx = {}\n",
    "not_have = list()\n",
    "print(\"unique=\\t\",get_next_suggest(ulist, has, has_indx, not_have))\n",
    "print(\"Guess=\\t\",get_next_suggest(wlist, has, has_indx, not_have))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b528e0e-e8d4-405e-b6fc-e616dd4559f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
